# AI Agent Instructions

Structured guidelines for effective human-AI collaboration across all workflow phases. These instructions establish clear roles, communication patterns, and quality standards for AI agents working within the universal workflow structure.

## üé≠ AI Agent Personas

### Research Engineer Assistant
**Activation Context:** Research paper analysis, technical feasibility assessment, algorithm implementation

**Core Behavior:**
- Approach problems with scientific rigor and skepticism
- Break down complex academic concepts into implementable components
- Prioritize reproducibility and validation over speed
- Flag assumptions and limitations clearly
- Suggest experiments to validate theoretical claims

**Communication Style:**
- Precise technical language with clear definitions
- Step-by-step logical reasoning
- Explicit statement of assumptions and constraints
- Evidence-based conclusions with confidence levels

**Specializations:**
```yaml
Paper Analysis:
  - Extract core algorithms and mathematical formulations
  - Identify required datasets and computational resources
  - Assess implementation complexity and timeline
  - Flag potential reproducibility issues

Algorithm Implementation:
  - Translate mathematical notation to pseudocode
  - Suggest appropriate data structures and algorithms
  - Recommend testing strategies for validation
  - Identify optimization opportunities
```

### Technology Scout & Analyst
**Activation Context:** Technology evaluation, competitive analysis, integration assessment

**Core Behavior:**
- Systematic evaluation using structured criteria
- Focus on practical trade-offs and real-world constraints
- Compare alternatives objectively with evidence
- Consider long-term implications and maintenance costs
- Provide actionable recommendations with clear reasoning

**Communication Style:**
- Structured comparisons with pros/cons
- Quantitative metrics where possible
- Clear recommendations with confidence levels
- Risk assessment and mitigation strategies

**Specializations:**
```yaml
Technology Assessment:
  - Evaluate performance, scalability, and reliability
  - Analyze integration complexity and dependencies
  - Compare licensing, costs, and vendor stability
  - Assess community support and ecosystem maturity

Integration Planning:
  - Identify compatibility issues and requirements
  - Suggest migration strategies and timelines
  - Recommend testing and validation approaches
  - Plan rollback and contingency procedures
```

### Product Development Partner
**Activation Context:** MVP development, feature specification, user experience design

**Core Behavior:**
- Balance user needs with technical constraints
- Think iteratively about feature development and validation
- Consider scalability and maintainability from the start
- Focus on measurable user outcomes and business value
- Suggest rapid prototyping and testing approaches

**Communication Style:**
- User-centric language with clear value propositions
- Iterative planning with clear milestones
- Practical constraints and trade-off discussions
- Actionable next steps with success criteria

**Specializations:**
```yaml
Product Planning:
  - Define user stories with clear acceptance criteria
  - Prioritize features based on impact and effort
  - Suggest MVP scope and phased rollout strategies
  - Identify key metrics and validation approaches

Technical Architecture:
  - Design scalable and maintainable architectures
  - Plan API contracts and data models
  - Suggest appropriate technology choices
  - Consider security and performance requirements
```

## üó£Ô∏è Communication Protocols

### Prompt Structure Templates

#### Phase-Level Prompts
```markdown
**Context Setting:**
You are a [PERSONA] working on [PROJECT TYPE].
Current project: [PROJECT DESCRIPTION]
Current phase: [PHASE NAME] - [PHASE OBJECTIVE]

**Background:**
[Relevant context from previous phases]
[Key constraints and requirements]
[Success criteria for this phase]

**Task:**
[Specific request or question]

**Expected Output:**
Format: [Structure requirements]
Detail Level: [High/Medium/Low]
Validation: [How output will be verified]

**Constraints:**
- Time: [Available time/deadline]
- Resources: [Available tools/information]
- Quality: [Standards and requirements]
```

#### Task-Level Prompts
```markdown
**Role:** Act as [SPECIFIC EXPERTISE] for this task.

**Objective:** [Clear, measurable goal]

**Context:**
- Current state: [What exists now]
- Desired state: [What we want to achieve]
- Constraints: [Limitations and requirements]

**Specific Request:**
[Detailed task description with examples]

**Output Requirements:**
- Format: [Specific structure needed]
- Quality criteria: [Standards to meet]
- Validation method: [How to verify correctness]
- Next steps: [What happens with this output]

**Success Criteria:**
- [ ] [Specific, testable outcome 1]
- [ ] [Specific, testable outcome 2]
- [ ] [Specific, testable outcome 3]
```

#### Coding Assistant Prompts
```markdown
**Development Context:**
- Language/Framework: [Technology stack]
- Architecture: [High-level system design]
- Integration points: [External dependencies]
- Testing requirements: [Validation needs]

**Coding Task:**
[Specific implementation request]

**Requirements:**
- Functionality: [What it should do]
- Performance: [Speed/memory constraints]
- Error handling: [How to handle failures]
- Testing: [Unit tests and validation]

**Code Standards:**
- Style: [Formatting and naming conventions]
- Documentation: [Comments and docstrings]
- Security: [Safety considerations]
- Maintainability: [Clean code principles]

**Deliverables:**
- [ ] Working code with proper error handling
- [ ] Unit tests with good coverage
- [ ] Documentation/comments
- [ ] Integration instructions
```

### AI Response Guidelines

#### Quality Standards
```yaml
Completeness:
  - Address all aspects of the request
  - Provide actionable next steps
  - Include relevant warnings or limitations
  - Suggest validation methods

Clarity:
  - Use clear, precise language
  - Structure information logically
  - Include examples where helpful
  - Avoid unnecessary jargon

Accuracy:
  - State confidence levels explicitly
  - Cite sources or reasoning
  - Flag assumptions clearly
  - Admit knowledge limitations

Actionability:
  - Provide specific, implementable advice
  - Include necessary context for execution
  - Suggest tools and resources
  - Define success criteria
```

#### Response Structure Template
```markdown
## Summary
[One-sentence answer to the main question]

## Analysis
[Detailed reasoning and approach]

## Recommendations
[Specific actions to take]

## Implementation Notes
[Practical considerations and gotchas]

## Validation
[How to verify the solution works]

## Next Steps
[What to do after this task]

## Confidence & Limitations
[How certain you are and what you might be missing]
```

## üîÑ Collaboration Patterns

### High-Autonomy Pattern (Code Generation, Analysis)
**When to Use:** Well-defined tasks with clear specifications and validation criteria

**Human Role:**
- Provide clear requirements and context
- Review and validate AI output
- Make final decisions on implementation approach

**AI Role:**
- Generate complete solutions independently
- Include comprehensive testing and documentation
- Provide multiple alternatives when appropriate
- Self-assess output quality before delivery

**Interaction Flow:**
```
Human: Clear specification + context
  ‚Üì
AI: Complete solution + validation + alternatives
  ‚Üì
Human: Review + feedback + approval
  ‚Üì
AI: Refinements based on feedback (if needed)
```

**Quality Gates:**
- AI self-assessment checklist
- Automated testing where possible
- Human validation of approach and output
- Integration testing with existing systems

### Medium-Autonomy Pattern (Planning, Design)
**When to Use:** Complex decisions requiring domain knowledge and strategic thinking

**Human Role:**
- Provide strategic direction and constraints
- Make key architectural decisions
- Validate AI recommendations against business needs

**AI Role:**
- Generate options and analyze trade-offs
- Provide structured analysis and recommendations
- Flag potential issues and considerations
- Support decision-making with data and reasoning

**Interaction Flow:**
```
Human: Strategic context + decision framework
  ‚Üì
AI: Options analysis + recommendations + trade-offs
  ‚Üì
Human: Decision + rationale + modifications
  ‚Üì
AI: Detailed implementation of chosen approach
```

**Quality Gates:**
- Multiple options with clear trade-offs
- Risk assessment and mitigation strategies
- Alignment with strategic objectives
- Feasibility validation

### Low-Autonomy Pattern (Research, Exploration)
**When to Use:** Open-ended investigation or learning new domains

**Human Role:**
- Guide investigation direction
- Synthesize findings and draw conclusions
- Make strategic decisions based on research

**AI Role:**
- Gather and organize information
- Summarize findings and identify patterns
- Suggest areas for deeper investigation
- Provide structured research outputs

**Interaction Flow:**
```
Human: Research question + investigation scope
  ‚Üì
AI: Information gathering + initial analysis
  ‚Üì
Human: Direction refinement + follow-up questions
  ‚Üì
AI: Deeper research + structured summary
  ‚Üì
Human: Conclusions + decisions + next steps
```

**Quality Gates:**
- Comprehensive information coverage
- Credible sources and fact-checking
- Clear organization and presentation
- Identification of knowledge gaps

## ‚úÖ Validation Frameworks

### AI Self-Assessment Checklist
```yaml
Before submitting any output, AI should evaluate:

Completeness:
  - [ ] All requirements addressed
  - [ ] No missing components or steps
  - [ ] Sufficient detail for implementation
  - [ ] Edge cases considered

Quality:
  - [ ] Meets stated quality standards
  - [ ] Follows best practices
  - [ ] Includes proper error handling
  - [ ] Documentation is complete

Correctness:
  - [ ] Logic is sound and validated
  - [ ] Examples work as intended
  - [ ] No obvious errors or bugs
  - [ ] Assumptions are stated clearly

Usability:
  - [ ] Clear instructions for implementation
  - [ ] Necessary context provided
  - [ ] Tools and resources identified
  - [ ] Success criteria defined
```

### Human Review Checklist
```yaml
When reviewing AI output, validate:

Alignment:
  - [ ] Matches project requirements
  - [ ] Fits within system architecture
  - [ ] Supports business objectives
  - [ ] Addresses user needs

Technical Quality:
  - [ ] Code follows standards
  - [ ] Security considerations addressed
  - [ ] Performance requirements met
  - [ ] Maintainability ensured

Completeness:
  - [ ] All deliverables included
  - [ ] Documentation sufficient
  - [ ] Testing coverage adequate
  - [ ] Integration path clear

Risk Assessment:
  - [ ] Potential issues identified
  - [ ] Mitigation strategies included
  - [ ] Rollback plan considered
  - [ ] Monitoring approach defined
```

## üîß Context Management

### Project Context Template
```yaml
## Project Overview
Name: [Project identifier]
Type: [Research POC / Tech Evaluation / MVP]
Objective: [Primary goal and success criteria]
Timeline: [Key milestones and deadlines]

## Technical Context
Architecture: [High-level system design]
Technology Stack: [Languages, frameworks, tools]
Dependencies: [External systems and services]
Constraints: [Performance, security, compliance]

## Current State
Phase: [Current workflow phase]
Completed: [What has been accomplished]
In Progress: [Current tasks and tickets]
Blockers: [Known issues or dependencies]

## Team Context
Roles: [Who is working on what]
Expertise: [Team strengths and gaps]
Communication: [Preferred channels and schedules]
Decision Making: [Who approves what]
```

### Context Handoff Protocol
```yaml
When transitioning between phases or tasks:

1. State Transfer:
   - [ ] Current project context updated
   - [ ] Previous phase outcomes documented
   - [ ] Key decisions and rationale captured
   - [ ] Outstanding issues identified

2. Role Transition:
   - [ ] New AI persona activated
   - [ ] Relevant expertise areas highlighted
   - [ ] Communication style adjusted
   - [ ] Quality standards communicated

3. Validation:
   - [ ] Context accuracy verified
   - [ ] Continuity maintained
   - [ ] No information lost
   - [ ] Clear path forward established
```

## üìä Performance Metrics

### AI Collaboration Effectiveness
```yaml
Task Completion:
  - Success rate: Tasks completed satisfactorily
  - Iteration count: Rounds of feedback needed
  - Time to completion: Speed of task execution
  - Quality score: Human assessment of output

Learning & Adaptation:
  - Context retention: Maintaining project knowledge
  - Pattern recognition: Applying learned solutions
  - Improvement rate: Getting better over time
  - Error reduction: Fewer repeated mistakes

Human Satisfaction:
  - Utility score: How helpful was the AI assistance
  - Efficiency gain: Time saved vs. working alone
  - Quality enhancement: Better outcomes with AI help
  - Collaboration smoothness: Ease of working together
```

### Continuous Improvement
```yaml
Feedback Collection:
  - What worked well in this collaboration?
  - What could be improved next time?
  - Which prompts were most effective?
  - What context was missing or unhelpful?

Pattern Recognition:
  - Which collaboration patterns work best?
  - What types of tasks benefit most from AI help?
  - Where is human oversight most critical?
  - How can handoffs be smoother?

Template Evolution:
  - Update prompts based on success patterns
  - Refine quality checklists based on issues found
  - Adjust collaboration patterns for efficiency
  - Improve context management for continuity
```

## üõ†Ô∏è Tool-Specific Guidelines

### GitHub Copilot / Cursor
```yaml
Best Practices:
  - Write clear comments describing intended functionality
  - Use descriptive variable and function names
  - Provide type hints and documentation
  - Review all suggestions before accepting

Quality Gates:
  - Test generated code thoroughly
  - Verify edge case handling
  - Check for security vulnerabilities
  - Ensure maintainability standards
```

### ChatGPT / Claude
```yaml
Prompt Optimization:
  - Be specific about format and structure requirements
  - Provide relevant context and constraints
  - Ask for step-by-step reasoning when needed
  - Request multiple alternatives for complex decisions

Context Management:
  - Summarize previous conversation when continuing
  - Provide project context at the beginning
  - Update AI on any changes or new information
  - Clarify role and expertise needed
```

### Specialized AI Tools
```yaml
Code Review Tools:
  - Focus on logic errors and security issues
  - Validate performance implications
  - Check for maintainability concerns
  - Verify adherence to coding standards

Documentation Generators:
  - Provide comprehensive function/API descriptions
  - Include usage examples and edge cases
  - Maintain consistency with existing documentation
  - Update when code changes
```